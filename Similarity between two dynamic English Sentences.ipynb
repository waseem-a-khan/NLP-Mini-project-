{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is very simple and mini-project to calculate the similarity between two sentences using Natural Language Processing(NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import some useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "#!pip install Unidecode\n",
    "import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example:-\n",
    "#sentence1 = \" The symbol # is commonly used with numbers, especially in American English.\"\n",
    "#sentence2 = \"We use the symbol % to indicate a percentage (that is, an amount in 100)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter first string: The symbol # is commonly used with numbers, especially in American English.\n",
      "Enter second string: We use the symbol % to indicate a percentage (that is, an amount in 100)\n"
     ]
    }
   ],
   "source": [
    "sentence1 = input(\"Enter first string: \").lower()\n",
    "sentence2 = input(\"Enter second string: \").lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing(Tokenization+Cleaning)of text input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "def remove_other_symbol(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]','',str(text))\n",
    "    cleaned_text = re.sub(r'\\d+','',str(cleaned_text))\n",
    "    cleaned_text=cleaned_text.lower()\n",
    "    #tokenize_word = re.split('\\W+',(cleaned_text))\n",
    "    #new_cleaned_text = [word for word in tokenize_word if word not in stopwords]\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To remove special Characters and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the symbol  is commonly used with numbers especially in american english\n",
      "----------------------------------------------------------------------------------\n",
      "we use the symbol  to indicate a percentage that is an amount in \n"
     ]
    }
   ],
   "source": [
    "cleaned_sent1 = remove_other_symbol(sentence1)\n",
    "print(cleaned_sent1)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "cleaned_sent2 = remove_other_symbol(sentence2)\n",
    "print(cleaned_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of two different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenize(txt):\n",
    "    tokenized_txt=re.split('\\W+',(txt))\n",
    "    return tokenized_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'symbol', 'is', 'commonly', 'used', 'with', 'numbers', 'especially', 'in', 'american', 'english']\n",
      "------------------------------------------------------------------------------------------\n",
      "['we', 'use', 'the', 'symbol', 'to', 'indicate', 'a', 'percentage', 'that', 'is', 'an', 'amount', 'in', '']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent1 = text_tokenize(cleaned_sent1)\n",
    "print(tokenized_sent1)\n",
    "print(\"------------------------------------------------------------------------------------------\")\n",
    "tokenized_sent2 = text_tokenize(cleaned_sent2)\n",
    "print(tokenized_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stopwords from the tokenized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i , me , my , myself , we , our , ours , ourselves , you , you're , you've , you'll , you'd , your , yours , yourself , yourselves , he , him , his , himself , she , she's , her , hers , herself , it , it's , its , itself , they , them , their , theirs , themselves , what , which , who , whom , this , that , that'll , these , those , am , is , are , was , were , be , been , being , have , has , had , having , do , does , did , doing , a , an , the , and , but , if , or , because , as , until , while , of , at , by , for , with , about , against , between , into , through , during , before , after , above , below , to , from , up , down , in , out , on , off , over , under , again , further , then , once , here , there , when , where , why , how , all , any , both , each , few , more , most , other , some , such , no , nor , not , only , own , same , so , than , too , very , s , t , can , will , just , don , don't , should , should've , now , d , ll , m , o , re , ve , y , ain , aren , aren't , couldn , couldn't , didn , didn't , doesn , doesn't , hadn , hadn't , hasn , hasn't , haven , haven't , isn , isn't , ma , mightn , mightn't , mustn , mustn't , needn , needn't , shan , shan't , shouldn , shouldn't , wasn , wasn't , weren , weren't , won , won't , wouldn , wouldn't\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Waseem Akram\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords=nltk.corpus.stopwords.words(\"english\")\n",
    "print(\" , \".join(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(tokenized_word):\n",
    "    remove_stopword=[word for word in tokenized_word if word not in stopwords]\n",
    "    return remove_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['symbol', 'commonly', 'used', 'numbers', 'especially', 'american', 'english']\n",
      "------------------------------------------------------------------------------------------\n",
      "['use', 'symbol', 'indicate', 'percentage', 'amount', '']\n"
     ]
    }
   ],
   "source": [
    "remove_sw_sent1 = remove_sw(tokenized_sent1)\n",
    "print(remove_sw_sent1)\n",
    "print(\"------------------------------------------------------------------------------------------\")\n",
    "remove_sw_sent2 = remove_sw(tokenized_sent2)\n",
    "print(remove_sw_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To lemmatization each sentence1 and Sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn=nltk.WordNetLemmatizer()\n",
    "def lemmatization(token_words):\n",
    "    lemmatized_txt=[wn.lemmatize(word) for word in token_words]\n",
    "    return lemmatized_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Waseem Akram\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['symbol', 'commonly', 'used', 'number', 'especially', 'american', 'english']\n",
      "------------------------------------------------------------------------------------------\n",
      "['use', 'symbol', 'indicate', 'percentage', 'amount', '']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lematized_sent1 = lemmatization(remove_sw_sent1)\n",
    "print(lematized_sent1)\n",
    "print(\"------------------------------------------------------------------------------------------\")\n",
    "lematized_sent2 = lemmatization(remove_sw_sent2)\n",
    "print(lematized_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol commonly used number especially american english\n",
      "---------------------------------------------------------------------------------------------\n",
      "use symbol indicate percentage amount \n"
     ]
    }
   ],
   "source": [
    "complete_cleaned_sent1 = \" \".join(lematized_sent1)\n",
    "print(complete_cleaned_sent1)\n",
    "print(\"---------------------------------------------------------------------------------------------\")\n",
    "complete_cleaned_sent2 = \" \".join(lematized_sent2)\n",
    "print(complete_cleaned_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To form a single Corpus from two different sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [complete_cleaned_sent1, complete_cleaned_sent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To convert the sentences into vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'symbol': 8, 'commonly': 2, 'used': 10, 'number': 6, 'especially': 4, 'american': 0, 'english': 3, 'use': 9, 'indicate': 5, 'percentage': 7, 'amount': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer1_2 = TfidfVectorizer()\n",
    "features_matrix = vectorizer1_2.fit_transform(corpus)\n",
    "print(vectorizer1_2.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Compute the Cosine Similarity between two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Cosine Similarity betweeen the two given sentences-------\n",
      "Cosine Similarity between two sentences: 0.09349477497536716\n"
     ]
    }
   ],
   "source": [
    "cos_sim = cosine_similarity(features_matrix)\n",
    "print(\"--------Cosine Similarity betweeen the two given sentences-------\")\n",
    "print(\"Cosine Similarity between two sentences:\", cos_sim[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Distance between two sentences: 0.9065052250246328\n"
     ]
    }
   ],
   "source": [
    "cosine_distance = 1-cos_sim[0][1]\n",
    "print(\"Cosine Distance between two sentences:\",cosine_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
